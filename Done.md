## Objectifs Accomplished:
- Code a Bigram Language Model architecture :heavy_check_mark:

- Code a Decoder Transformer architecture :heavy_check_mark:

- Data processing of tiny Shakespeare dataset :heavy_check_mark:

- Tokenizer to encode the dataset :heavy_check_mark:

- Add bite pair encoding algorithm to improve the tokenizer  :heavy_check_mark:

- Training GPU :heavy_check_mark:

- Adaptation to lightning :heavy_check_mark: 
	- Data loader :heavy_check_mark:

	- Passer les models sous lightning :heavy_check_mark:
	 	- Bigram :heavy_check_mark:
		- Transformer :heavy_check_mark:
		- generic lightning trainer :heavy_check_mark:

	- Nomage et sauvegarde des Models :heavy_check_mark:
		- Model/Training Namining/Versionning :heavy_check_mark:
		- Convention de nomage pour le version des modèles :heavy_check_mark:
		- Dynamique Model/Training Namining/Versionning :heavy_check_mark:

	- Gestion des entrainements :heavy_check_mark:
		- train and test split :heavy_check_mark:
		- Record metric during training :heavy_check_mark:
		- Model save end training :heavy_check_mark:
		- Save hyper-parameters of the training :heavy_check_mark:
	
	- Pass forward avec model entrainé et sauvegardé :heavy_check_mark:

- Document code :heavy_check_mark:
	- BitePairEncoding :heavy_check_mark:
	- Data :heavy_check_mark:
	- Loss :heavy_check_mark:
	- Tokenizer :heavy_check_mark:
